1. SAST:
2. DAST:
Sonarcube has 2 components:
1. Sonarqube Server: Place where report will be published.
2. Sonarqube Scanner: Report is generated by this.

kubectl delete pod frontend-ci-19-n1cvl-6rc0r-8k11h \
  -n jenkins \
  --grace-period=0 \
  --force


helm uninstall sonarqube -n sonarqube
kubectl get all -n sonarqube
kubectl delete pod sonarqube-sonarqube-0 \
  -n sonarqube \
  --grace-period=0 \
  --force
kubectl delete pod -n sonarqube --all --force --grace-period=0
helm install sonarqube sonarqube/sonarqube \
  -n sonarqube \
  --create-namespace \
  -f values.yaml


kind create cluster --name devsecops
kubectl config get-contexts
kubectl config use-context kind-devsecops
helm repo add jenkins https://charts.jenkins.io
helm repo update
helm install jenkins jenkins/jenkins   -n jenkins   -f jenkins-values.yaml
kubectl get pods -n jenkins -w 
kubectl port-forward svc/jenkins -n jenkins 8081:8080

helm repo add sonarqube https://SonarSource.github.io/helm-chart-sonarqube 
helm repo update
kubectl create ns sonarqube
kubectl create secret generic sonarqube-monitoring   -n sonarqube   --from-literal=monitoringPasscode=sonar123
helm install sonarqube sonarqube/sonarqube   -n sonarqube   --create-namespace   -f sonar-values.yaml
kubectl get pods -n sonarqube -w
kubectl port-forward svc/sonarqube-sonarqube 9000:9000 -n sonarqube


### Step 1: Create OIDC Provider in Terraform
*Inside modules/eks/main.tf.*  
```
data "aws_eks_cluster" "cluster" {
  name = aws_eks_cluster.eks_cluster.name
}

data "aws_eks_cluster_auth" "cluster" {
  name = aws_eks_cluster.eks_cluster.name
}

resource "aws_iam_openid_connect_provider" "eks_oidc" {
  url = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer

  client_id_list = ["sts.amazonaws.com"]

  thumbprint_list = ["9e99a48a9960b14926bb7f3b02e22da0ecd2b6c3"]
}
```
*But when we will install Jenkins using Terraform, above two (data) will be shifted to root main.tf file.*   
*Also, if data is shifted to root main.tf file, then url will change to "url = aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer"*


### Step2: Create IAM Policy for ALB Controller
*Create a new file modules/eks/alb_iam.tf*  
```
resource "aws_iam_policy" "alb_controller_policy" {
  name = "${var.cluster_name}-AWSLoadBalancerControllerIAMPolicy"

  policy = file("${path.module}/iam_policy.json")
}
```

**Now download official policy file from inside modules/eks:**
```
curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.11.0/docs/install/iam_policy.json
```

### Step3: Create IAM Role for Service Account (inside alb_iam.tf)
```
data "aws_iam_policy_document" "alb_assume_role_policy" {
  statement {
    effect = "Allow"

    actions = ["sts:AssumeRoleWithWebIdentity"]

    principals {
      type        = "Federated"
      identifiers = [aws_iam_openid_connect_provider.eks_oidc.arn]
    }

    condition {
      test     = "StringEquals"
      variable = "${replace(aws_iam_openid_connect_provider.eks_oidc.url, "https://", "")}:sub"

      values = [
        "system:serviceaccount:kube-system:aws-load-balancer-controller"
      ]
    }
  }
}

resource "aws_iam_role" "alb_controller_role" {
  name = "${var.cluster_name}-AmazonEKSLoadBalancerControllerRole"

  assume_role_policy = data.aws_iam_policy_document.alb_assume_role_policy.json
}

resource "aws_iam_role_policy_attachment" "alb_attach" {
  role       = aws_iam_role.alb_controller_role.name
  policy_arn = aws_iam_policy.alb_controller_policy.arn
}
```

*If data is shifted to root main.tf file, then variable will change to -: variable = "${replace(aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer, "https://", "")}:sub"*

### Step4: Add Output (inside module/eks/outputs.tf)
```
output "alb_controller_role_arn" {
  value = aws_iam_role.alb_controller_role.arn
}
```

### Step 5: Add Output (inside EKS/outputs.tf)
```
output "alb_controller_role_arn" {
  value = module.eks.alb_controller_role_arn
}
```

### Step 6: Apply terraform

## Now bind this IAM role to Kubenetes using IRSA

### Step 1: Get the IAM Role ARN
```
terraform output alb_controller_role_arn
```

## Create Kubernetes Service Account (IIRSA Binding)
**Create a file in project root folder under k8/platform/aws-load-balancer-controller/serviceaccount.yaml**
```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aws-load-balancer-controller
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: <IAM_ROLE_ARN>
```
*Now run*
```
kubectl apply -f k8/platform/aws-load-balancer-controller/serviceaccount.yaml
```

## Install AWS Load Balancer Controller

### Step 1: Add Helm Repo
```
helm repo add eks https://aws.github.io/eks-charts
helm repo update
```

### Step 2: Install AWS Load Balancer Controller
```
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=devsecops-eks-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=ap-south-1 \
  --set vpcId=<your-vpc-id>


```
arn:aws:iam::202749265471:role/devsecops-eks-cluster-AmazonEKSLoadBalancerControllerRole

## PHASE: Deploy your applications

### Step 1: Create Namespace
```
kubectl create ns argocd
kubectl create ns dev 
```
*This namespace should match the destination namespace of application yaml files.*

### Step 2: Install Argocd
```
kubectl apply -n argocd   -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

### Step 2: Apply your Yamls
```
kubectl apply -f mongodb/application.yaml
kubectl apply -f backend/application.yaml
kubectl apply -f frontend/application.yaml
kubectl apply -f ingress.yaml
```

## Phase Install Jenkins using Terraform

### Step 1: Add Helm + Kubernetes Providers in Root providers.tf
```
provider "kubernetes" {
  host                   = module.eks.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
  token                  = data.aws_eks_cluster_auth.cluster.token
}

provider "helm" {
  kubernetes = {
    host = module.eks.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
    token = data.aws_eks_cluster_auth.cluster.token
  }
}
```

### Step 3: Create Jenkins Namespace via Terraform and Add Helm Release for Jenkins inside modules/jenkins/main.tf
```
resource "kubernetes_namespace_v1" "jenkins" {
  metadata {
    name = "jenkins"
  }
}

resource "helm_release" "jenkins" {
  name       = "jenkins"
  namespace  = kubernetes_namespace_v1.jenkins.metadata[0].name
  repository = "https://charts.jenkins.io"
  chart      = "jenkins"

  values = [
    file("${path.module}/values.yaml")
  ]
}

```

### Step 4: Add this values.yaml in module/jenkins
```
controller:
  admin:
    createSecret: true
    username: admin
    password: admin123

  serviceType: ClusterIP

  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1"
      memory: "1Gi"

  installPlugins:
    - kubernetes
    - workflow-aggregator
    - git
    - configuration-as-code
    - credentials
    - sonar
    - docker-workflow

persistence:
  enabled: true
  size: 8Gi
  storageClass: gp2
```

### Step 5: Add this to root main.tf file
```
data "aws_eks_cluster" "cluster" {
  name = module.eks.cluster_name
}

data "aws_eks_cluster_auth" "cluster" {
  name = module.eks.cluster_name
}

module "jenkins" {
  source = "./modules/jenkins"

  depends_on = [ module.eks ]
}
```

### Step 6: Run
```
terraform init
terraform apply
kubectl get pods -n jenkins
```

### Step 7: Expose jenkins using ingress. Create jenkins-ingress.yaml inside k8/jenkins
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jenkins-ingress
  namespace: jenkins
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: jenkins
                port:
                  number: 8080
```

### Step 8: Run
```
kubectl apply -f jenkins-ingress.yaml
kubectl get ingress jenkins-ingress -n jenkins
kubectl get svc -n jenkins
```
*you will get a link like this "http://k8s-jenkins-jenkinsi-c237ef4fb5-316841084.ap-south-1.elb.amazonaws.com/" open in browser.*

## Add Sonarqube 

### Step 1: Create Sonarqube Namespace via Terraform and Add Helm Release for Sonarqube inside modules/sonarqube/main.tf
```
resource "kubernetes_namespace_v1" "sonarqube" {
  metadata {
    name = "sonarqube"
  }
}

resource "kubernetes_secret_v1" "sonarqube_monitoring" {
  metadata {
    name      = "sonarqube-monitoring"
    namespace = kubernetes_namespace_v1.sonarqube.metadata[0].name
  }

  data = {
    monitoringPasscode = base64encode("sonar123")
  }
}

resource "helm_release" "sonarqube" {
  name       = "sonarqube"
  namespace  = kubernetes_namespace_v1.sonarqube.metadata[0].name
  repository = "https://SonarSource.github.io/helm-chart-sonarqube"
  chart      = "sonarqube"

  values = [
    file("${path.module}/values.yaml")
  ]

  depends_on = [ kubernetes_secret_v1.sonarqube_monitoring ]
}

```

### Step 2: Add this values.yaml in module/sonarqube
```
community:
  enabled: true

image:
  repository: sonarqube
  tag: 25.12.0.117093-community
  pullPolicy: IfNotPresent

postgresql:
  enabled: true
  auth:
    username: sonar
    password: sonar
    database: sonarqube

persistence:
  enabled: true
  size: 10Gi
  storageClass: gp2

monitoringPasscodeSecretName: sonarqube-monitoring
monitoringPasscodeSecretKey: monitoringPasscode

resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "1"
    memory: "2Gi"
```

### Step 3: Add this to root main.tf file
```
module "sonarqube" {
  source = "./modules/sonarqube"

  depends_on = [ module.eks ]
}
```

### Step 4: Run
```
terraform init
terraform apply
kubectl get pods -n sonarqube
```

### Step 5: Expose sonarqube using ingress. Create sonarqube-ingress.yaml inside k8/sonarqube
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: sonarqube-ingress
  namespace: sonarqube
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: sonarqube-sonarqube
                port:
                  number: 9000

```

### Step 6: Run
```
kubectl apply sonarqube-ingress.yaml
kubectl get ingress sonarqube-ingress -n sonarqube
kubectl get svc -n sonarqube
```
*you will get a link like this "http://k8s-jenkins-jenkinsi-c237ef4fb5-316841084.ap-south-1.elb.amazonaws.com/" open in browser.*


### Step 7: Create Dockerhub secret for Kaniko
```
kubectl create secret docker-registry dockerhub-secret \
  --docker-username=<your-username> \
  --docker-password=<your-password> \
  --docker-email=<your-email> \
  -n jenkins

```

### Step 8: Run the pipeline

## Phase : Argocd application

### Create Argocd ingress under k8/argocd/argocd-ingress.yaml
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argocd-ingress
  namespace: argocd
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/backend-protocol: HTTP
spec:
  ingressClassName: alb
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: argocd-server
            port:
              number: 80
```
kubectl apply argocd-ingress.yaml
kubectl get ingress -n argocd
kubectl get svc -n argocd

*this yaml is wrong. i used port forward here instead.*

sk1-TKPVGgrPGyE5






ritesh@LAPTOP-BEPRR95P:/mnt/c/Users/rites/Desktop/DRP 2/End-to-End-Kubernetes-Three-Tier-DevSecOps-Project/k8/sonarqube$ kubectl get nodes -A
NAME                                       STATUS   ROLES    AGE   VERSION
ip-10-0-3-56.ap-south-1.compute.internal   Ready    <none>   28h   v1.34.3-eks-70ce843
ip-10-0-4-79.ap-south-1.compute.internal   Ready    <none>   29m   v1.34.3-eks-70ce843
ritesh@LAPTOP-BEPRR95P:/mnt/c/Users/rites/Desktop/DRP 2/End-to-End-Kubernetes-Three-Tier-DevSecOps-Project/k8/sonarqube$ kubectl get pods
No resources found in default namespace.
ritesh@LAPTOP-BEPRR95P:/mnt/c/Users/rites/Desktop/DRP 2/End-to-End-Kubernetes-Three-Tier-DevSecOps-Project/k8/sonarqube$ kubectl get pods -A
NAMESPACE     NAME                                                READY   STATUS             RESTARTS          AGE
argocd        argocd-application-controller-0                     1/1     Running            0                 23h
argocd        argocd-applicationset-controller-5796dcfc94-xqq2l   0/1     CrashLoopBackOff   197 (3m57s ago)   23h
argocd        argocd-dex-server-6d57f6f6b6-l4dd9                  1/1     Running            0                 23h
argocd        argocd-notifications-controller-bb4f97f47-2d8lj     1/1     Running            0                 23h
argocd        argocd-redis-6dfddccb76-4gmgq                       1/1     Running            0                 23h
argocd        argocd-repo-server-77d887cfb9-dx96f                 1/1     Running            0                 23h
argocd        argocd-server-7fb8c5f74-ct2vb                       1/1     Running            0                 23h
dev           backend-587864f54-q8tpr                             1/1     Running            0                 23h
dev           frontend-7b9bf5f5f4-dvk4r                           1/1     Running            0                 23h
dev           mongodb-56f7d756b4-h2czj                            1/1     Running            0                 21h
jenkins       frontend-ci-1-15bkq-f9xc6-48l06                     6/6     Running            0                 117s
jenkins       jenkins-0                                           2/2     Running            0                 3h13m
kube-system   aws-load-balancer-controller-dbd98cfc5-nwvpv        1/1     Running            0                 25h
kube-system   aws-load-balancer-controller-dbd98cfc5-p2sp5        1/1     Running            0                 25h
kube-system   aws-node-mdhg8                                      2/2     Running            0                 28h  
kube-system   aws-node-shdhs                                      2/2     Running            0                 30m  
kube-system   coredns-c57cc66d8-464tt                             1/1     Running            0                 28h  
kube-system   coredns-c57cc66d8-697gs                             1/1     Running            0                 28h  
kube-system   ebs-csi-controller-84f5c8d4c9-lwgg8                 6/6     Running            0                 18h  
kube-system   ebs-csi-controller-84f5c8d4c9-m8wlx                 6/6     Running            0                 18h  
kube-system   ebs-csi-node-8ljlg                                  3/3     Running            0                 18h  
kube-system   ebs-csi-node-fwzmn                                  3/3     Running            0                 30m  
kube-system   kube-proxy-h5slg                                    1/1     Running            0                 30m  
kube-system   kube-proxy-q7rxv                                    1/1     Running            0                 28h  
sonarqube     sonarqube-sonarqube-0 