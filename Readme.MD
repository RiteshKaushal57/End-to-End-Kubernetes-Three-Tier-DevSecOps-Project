1. SAST:
2. DAST:
Sonarcube has 2 components:
1. Sonarqube Server: Place where report will be published.
2. Sonarqube Scanner: Report is generated by this.

kubectl delete pod frontend-ci-19-n1cvl-6rc0r-8k11h \
  -n jenkins \
  --grace-period=0 \
  --force


helm uninstall sonarqube -n sonarqube
kubectl get all -n sonarqube
kubectl delete pod sonarqube-sonarqube-0 \
  -n sonarqube \
  --grace-period=0 \
  --force
kubectl delete pod -n sonarqube --all --force --grace-period=0
helm install sonarqube sonarqube/sonarqube \
  -n sonarqube \
  --create-namespace \
  -f values.yaml

taskkill /IM "Docker Desktop.exe" /F
taskkill /IM "com.docker.backend.exe" /F
taskkill /IM "com.docker.proxy.exe" /F

dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart

wsl --install --no-distribution
wsl --update

wsl --status

Get-Service *lxss*


kind create cluster --name devsecops
kubectl config get-contexts
kubectl config use-context kind-devsecops
helm repo add jenkins https://charts.jenkins.io
helm repo update
helm install jenkins jenkins/jenkins   -n jenkins   -f jenkins-values.yaml
kubectl get pods -n jenkins -w 
kubectl port-forward svc/jenkins -n jenkins 8081:8080

helm repo add sonarqube https://SonarSource.github.io/helm-chart-sonarqube 
helm repo update
kubectl create ns sonarqube
kubectl create secret generic sonarqube-monitoring   -n sonarqube   --from-literal=monitoringPasscode=sonar123
helm install sonarqube sonarqube/sonarqube   -n sonarqube   --create-namespace   -f sonar-values.yaml
kubectl get pods -n sonarqube -w
kubectl port-forward svc/sonarqube-sonarqube 9000:9000 -n sonarqube


### Step 1: Create OIDC Provider in Terraform
*Inside modules/eks/main.tf.*  
```
data "aws_eks_cluster" "cluster" {
  name = aws_eks_cluster.eks_cluster.name
}

data "aws_eks_cluster_auth" "cluster" {
  name = aws_eks_cluster.eks_cluster.name
}

resource "aws_iam_openid_connect_provider" "eks_oidc" {
  url = data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer

  client_id_list = ["sts.amazonaws.com"]

  thumbprint_list = ["9e99a48a9960b14926bb7f3b02e22da0ecd2b6c3"]
}
```
*But when we will install Jenkins using Terraform, above two (data) will be shifted to root main.tf file.*   
*Also, if data is shifted to root main.tf file, then url will change to "url = aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer"*


### Step2: Create IAM Policy for ALB Controller
*Create a new file modules/eks/alb_iam.tf*  
```
resource "aws_iam_policy" "alb_controller_policy" {
  name = "${var.cluster_name}-AWSLoadBalancerControllerIAMPolicy"

  policy = file("${path.module}/iam_policy.json")
}
```

**Now download official policy file from inside modules/eks:**
```
curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.11.0/docs/install/iam_policy.json
```

### Step3: Create IAM Role for Service Account (inside alb_iam.tf)
```
data "aws_iam_policy_document" "alb_assume_role_policy" {
  statement {
    effect = "Allow"

    actions = ["sts:AssumeRoleWithWebIdentity"]

    principals {
      type        = "Federated"
      identifiers = [aws_iam_openid_connect_provider.eks_oidc.arn]
    }

    condition {
      test     = "StringEquals"
      variable = "${replace(aws_iam_openid_connect_provider.eks_oidc.url, "https://", "")}:sub"

      values = [
        "system:serviceaccount:kube-system:aws-load-balancer-controller"
      ]
    }
  }
}

resource "aws_iam_role" "alb_controller_role" {
  name = "${var.cluster_name}-AmazonEKSLoadBalancerControllerRole"

  assume_role_policy = data.aws_iam_policy_document.alb_assume_role_policy.json
}

resource "aws_iam_role_policy_attachment" "alb_attach" {
  role       = aws_iam_role.alb_controller_role.name
  policy_arn = aws_iam_policy.alb_controller_policy.arn
}
```

*If data is shifted to root main.tf file, then variable will change to -: variable = "${replace(aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer, "https://", "")}:sub"*

### Step4: Add Output (inside module/eks/outputs.tf)
```
output "alb_controller_role_arn" {
  value = aws_iam_role.alb_controller_role.arn
}
```

### Step 5: Add Output (inside EKS/outputs.tf)
```
output "alb_controller_role_arn" {
  value = module.eks.alb_controller_role_arn
}
```

### Step 6: Apply terraform

## Now bind this IAM role to Kubenetes using IRSA

### Step 1: Get the IAM Role ARN
```
terraform output alb_controller_role_arn
```

## Create Kubernetes Service Account (IIRSA Binding)
**Create a file in project root folder under k8/platform/aws-load-balancer-controller/serviceaccount.yaml**
```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aws-load-balancer-controller
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: <IAM_ROLE_ARN>
```
*Now run*
```
kubectl apply -f k8/platform/aws-load-balancer-controller/serviceaccount.yaml
```

## Install AWS Load Balancer Controller

### Step 1: Add Helm Repo
```
helm repo add eks https://aws.github.io/eks-charts
helm repo update
```

### Step 2: Install AWS Load Balancer Controller
```
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=devsecops-eks-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=ap-south-1 \
  --set vpcId=<your-vpc-id>


```
arn:aws:iam::202749265471:role/devsecops-eks-cluster-AmazonEKSLoadBalancerControllerRole

## PHASE: Deploy your applications

### Step 1: Create Namespace
```
kubectl create ns argocd
kubectl create ns dev 
```
*This namespace should match the destination namespace of application yaml files.*

### Step 2: Install Argocd
```
kubectl apply -n argocd   -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

### Step 2: Apply your Yamls
```
kubectl apply -f mongodb/application.yaml
kubectl apply -f backend/application.yaml
kubectl apply -f frontend/application.yaml
kubectl apply -f ingress.yaml
```

## Phase Install Jenkins using Terraform

### Step 1: Add Helm + Kubernetes Providers in Root providers.tf
```
provider "kubernetes" {
  host                   = module.eks.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
  token                  = data.aws_eks_cluster_auth.cluster.token
}

provider "helm" {
  kubernetes = {
    host = module.eks.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
    token = data.aws_eks_cluster_auth.cluster.token
  }
}
```

### Step 3: Create Jenkins Namespace via Terraform and Add Helm Release for Jenkins inside modules/jenkins/main.tf
```
resource "kubernetes_namespace_v1" "jenkins" {
  metadata {
    name = "jenkins"
  }
}

resource "helm_release" "jenkins" {
  name       = "jenkins"
  namespace  = kubernetes_namespace_v1.jenkins.metadata[0].name
  repository = "https://charts.jenkins.io"
  chart      = "jenkins"

  values = [
    file("${path.module}/values.yaml")
  ]
}

```

### Step 4: Add this values.yaml in module/jenkins
```
controller:
  admin:
    createSecret: true
    username: admin
    password: admin123

  serviceType: ClusterIP

  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1"
      memory: "1Gi"

  installPlugins:
    - kubernetes
    - workflow-aggregator
    - git
    - configuration-as-code
    - credentials
    - sonar
    - docker-workflow

persistence:
  enabled: true
  size: 8Gi
  storageClass: gp2
```

### Step 5: Add this to root main.tf file
```
data "aws_eks_cluster" "cluster" {
  name = module.eks.cluster_name
}

data "aws_eks_cluster_auth" "cluster" {
  name = module.eks.cluster_name
}

module "jenkins" {
  source = "./modules/jenkins"

  depends_on = [ module.eks ]
}
```

### Step 6: Run
```
terraform init
terraform apply
kubectl get pods -n jenkins
```

### Step 7: Expose jenkins using ingress. Create jenkins-ingress.yaml inside k8/jenkins
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jenkins-ingress
  namespace: jenkins
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: jenkins
                port:
                  number: 8080
```

### Step 8: Run
```
kubectl apply -f jenkins-ingress.yaml
kubectl get ingress jenkins-ingress -n jenkins
kubectl get svc -n jenkins
```
*you will get a link like this "http://k8s-jenkins-jenkinsi-c237ef4fb5-316841084.ap-south-1.elb.amazonaws.com/" open in browser.*

### Create Dockerhub secret for Kaniko
```
kubectl create secret docker-registry dockerhub-secret \
  --docker-username=<your-username> \
  --docker-password=<your-password> \
  --docker-email=<your-email> \
  -n jenkins

```